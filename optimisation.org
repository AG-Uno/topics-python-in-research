* Numerical Optimisation
Suppose we have a functional $F$ and function $G$ defined as follows.
\begin{equation}
F[f] = \int_D \mathcal{L} dx 
\end{equation}
where the integration is over any domain $D$, multidimensional or not, infinite or not. \(\mathcal{L}\) is a
function of \(f(x)\) and its derivatives, but not directly of $x$. Typically there will not be any mixed
derivatives or derivatives of order higher than 2.

#+BEGIN_EXPORT latex :exports code :results value latex
\begin{equation}
G[g] = \sum_(i \in E) g_i
\end{equation}
#+END_EXPORT
where $g_i$ can also be thought of samples of some function at points $i$, i.e. \(g_i = g(x_i)\). 

We need to find $f$ ($g$) such that it (locally) minimises (or maximises) $F$ ($G$). Typically infinities in
the domain can be cut off numerically, but obviously $f$ will then need to approach zero quickly enough
to ensure this does not cause too big errors.

If $D$ is not a nice rectangular subset of \(\mathbb{R}^N\), the approach introduced is not very good: one
would need to embed $f$ into a rectangular domain and require \(f(x \ni D) = 0\), but this can lead to a very
complicated code or a code which spends most of its time adding together billions of zeros. A better approach
in these cases is to use a more sophisticated discretisation method, like the finite element method (FEM). A very
good FEM library with a natural python interface is [[https://fenicsproject.org][FEniCS]], Its python binding should be installable by =pip
install fenics= but be warned: it requires a large number of dependencies some of which =pip= may not be able
to install for you if you do not have them.

** Finite Differences
The simplest way of dealing with \(F\) is to write a finite difference approximation, where one replaces
differentials and integrals with their definitions without the \(\lim\), i.e.
\begin{equation}
\frac{df(x)}{dx} \to \frac{f(x+h)-f(x-h)}{2h}\\
\int_D f(x) dx \to \sum_{x \in D'} f(x) h.
\end{equation}
As an example, suppose \(\mathcal{L} = f'(x)^2/2 - \cos(f)\), so we can compute the Euler-Lagrange equation
of \(F[f]\):
\begin{align}
0 &= \frac{d}{dx} \frac{d\mathcal{L}}{d f'} - \frac{d\mathcal{L}}{df} \\
  &= f''(x) - \sin(f),
\end{align}
which is the (familiar?) sine-Gordon equation. Assuming boundary conditions \(f(-\infty)=0\) and
\(f(\infty)=2\pi\), it has a solution
\begin{align}
f(x) = 4 \arctan(\exp^{x-a}),
\end{align}
where \(a\) is a free integration constant (location of the peak of our soliton) but the other integration
constant was determined from the boundary conditions.

We want to find this numerically, using finite difference approximation. We will need to discretise the
objective function \(F[f]\). The discrete version of \(F[f]\) will have the form of \(G[g]\) and any sensible
algorithm will take advantage of either the Euler-Lagrange equation or the gradient (with respect to
\(g_i\)) of \(G[g]\). On the continuum (\(h \to 0\)) limit these will be equivalent, but numerically there are
subtle differences: using the Euler-Lagrange equation is slightly simpler to implement, but on the other hand
it only works in a case where you have \(\mathcal{L}\). Using the gradient of \(G[g]\) is slightly more
generic but even that will of course fail if \(G[g]\) is not differentiable. In this case, some sort of
regularisation is necessary.

There are also algorithms that do not use gradient information, but they are generally very slow. However,
they are also able to find a global minimum unlike gradient based methods which simply find the "closest"
local minimum which either is or is not a global minimum. Two of the most commonly used such algorithms are
/simulated annealing/ and /genetic algorithm/.

