#+TITLE: Introduction to Algorithms in Python
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper,10pt]
#+OPTIONS: H:3
#+OPTIONS: toc:1
* Efficient and portable I/O
- You will eventually want to read and write data, too
- *Badly implemented I/O can easily take 99% of your run-time!*
** Portable I/O
- plain text is portable, but
  - horribly space wasting (unless compressed, which then harms portability)
  - horribly slow as everything needs to be converted
    - unless your actual calculations are done using letters
  - cannot really be parallelised so in parallel even more slower
- some file formats will be hard or impossible to open on another computer so
- avoid machine dependent I/O, like
  - The unformatted IO in Fortran
    - unlikely to be efficient
    - likely to be very hard to open on another machine or Fortran library
  - C's =fwrite(data, size, count, file)=
    - with the right =size= and =count= this can be very efficient for a single-threaded application
    - but not very portable: few data types are guaranteed to be equivalent across machines and bit-order can
      also change
- some good libraries to portable I/O
  - HDF5 is de facto high-performance data format and very standardised
    - will *not* load the file to memory unless it is sliced or altered: useful for processing files which are
      too large for your memory if you do not need to alter the data
  - netcdf4 is also quite efficient (in fact it's HDF5 underneath)
  - sometimes numpy's =save=, =savez= and =savez_compressed= are ok
    - remember to pass =allow_pickle=False= to maintain portability
    - when loading, pass =mmap_mode= parameter to use memmaped IO: useful for large files when only part of it
      is needed
    - but have a look at [[http://blaze.readthedocs.io/en/latest/index.html][Blaze]], too
*** numpy can import almost any text file easily
- suppose you have a file like this 
#+BEGIN_SRC python :var filename="files/genfromtxt_example_data.txt" :exports results
  data='''# This file contains a bunch of point particles of varying locationg, speeds, masses and charges\nX\tY\tZ\tVx\tVy\tVz\tmass\tcharge\n0\t0\t0\t0\t0\t0.1\t100.0\t0.0\n1.0\t0\t4\t0\t10.0\t0\t10.0\t-1.0\n0\t2\t0\t0.1\t0\t0.1\t1000.0\t0.0\n2\t2\t-3\t0.1\t0\t0.1\t100.0\t1.0\n# A comment line'''
  with open(filename,"w") as f:
      f.write(data)
  return data
#+END_SRC

#+RESULTS:
: # This file contains a bunch of point particles of varying locationg, speeds, masses and charges
: X	Y	Z	Vx	Vy	Vz	mass	charge
: 0	0	0	0	0	0.1	100.0	0.0
: 1.0	0	4	0	10.0	0	10.0	-1.0
: 0	2	0	0.1	0	0.1	1000.0	0.0
: 2	2	-3	0.1	0	0.1	100.0	1.0
: # A comment line

- you can import this with numpy easily
#+BEGIN_SRC python :tangle yes :tangle "files/genfromtxt_example_import.py" :var filename="files/genfromtxt_example_data.txt"
  import numpy
  data = numpy.genfromtxt(filename, comments="#", delimiter="\t", skip_header=2)
  return data
#+END_SRC

#+RESULTS:
| 0 | 0 | 0 |   0 |  0 | 0.1 |   1 | 0 |
| 1 | 0 | 0 |   0 | 10 |   0 | 0.1 | 1 |
| 0 | 2 | 0 | 0.1 |  0 | 0.1 |  10 | 0 |

- please see =help(numpy.genfromtxt)= for full documentation: the function is capable of ingesting almost any
  kind of textual data, even strings!
- but it is not fast, no text import ever is

*** TODO!!! Jumping the gun here, but this is how you plot the genfromtxt example
#+BEGIN_SRC python :tangle yes :tangle "files/genfromtxt_example_plot.py" :var filename="files/genfromtxt_example_data.txt"
  import numpy
  import matplotlib
  import matplotlib.pyplot
  from mpl_toolkits.mplot3d import Axes3D

  def randrange(n, vmin, vmax):
      return (vmax - vmin)*numpy.random.rand(n) + vmin

  data = numpy.genfromtxt(filename, comments="#", delimiter="\t", skip_header=2)

  fig = matplotlib.pyplot.figure()
  ax = fig.add_subplot(111, projection='3d')
  n = data.shape[0]
  for particle in data[:]:
      # plot a sphere for each particle
      # colour charged particles red (charge>0), blue (charge<0) and neutrals green
      blues = data[data[:,7]<0]
      reds = data[data[:,7]>0]
      greens=data[numpy.logical_not(numpy.logical_or(data[:,7]<0,data[:,7]>0))]
      ax.scatter(blues[:,0], blues[:,1], blues[:,2], c="b", marker="o", s=blues[:,6])
      ax.scatter(reds[:,0], reds[:,1], reds[:,2], c="r", marker="o", s=greens[:,6])
      ax.scatter(greens[:,0], greens[:,1], greens[:,2], c="g", marker="o", s=greens[:,6])
      ax.quiver(blues[:,0], blues[:,1], blues[:,2], blues[:,3], blues[:,4], blues[:,5], pivot="tail")
      ax.quiver(reds[:,0], reds[:,1], reds[:,2], reds[:,3], reds[:,4], reds[:,5], pivot="middle")
      ax.quiver(greens[:,0], greens[:,1], greens[:,2], greens[:,3], greens[:,4], greens[:,5], pivot="tip")
  ax.set_xlabel('X Label')
  ax.set_ylabel('Y Label')
  ax.set_zlabel('Z Label')
  matplotlib.pyplot.show()
#+END_SRC
- TODO!!! how do we display this, jupyter or...?

** Performant I/O
- all the above formats can be internally and transparently compressed and it's nearly always a good idea
  - CPU time spent compressing is often more than saved in reduced time writing/reading to disc
  - not to mention less disc space needed
  - but compression is not magic (TODO!!! perhaps change this, it is not very good?):
#+BEGIN_SRC python :tangle yes :tangle "../codes/python/compressed_numpy.py"
  import numpy
  import tempfile
  import cProfile
  import pstats
  data=numpy.random.random((1000,1000,100))
  tempfiles = [tempfile.TemporaryFile(dir=".") for i in [0,1,2,3]]
  cps = [cProfile.Profile() for i in range(len(tempfiles))]
  runs = ["numpy.savez", "numpy.savez_compressed", "numpy.savez_compressed",
          "numpy.savez_compressed"]
  for i,r in enumerate(runs):
      if (i==2):
          data[100:900,100:900,30:70]=0.0
      if (i==3):
          data = numpy.ones((1000,1000,100), dtype=numpy.float64)
      cps[i].runcall(eval(r), tempfiles[i], {"array_called_data":data})

  print('''Time spent and file sizes:
    uncompressed random data:   {uncompt:g}\t{uncomps} 
    compressed random data:     {compt:g}\t{comps}
    compressed semirandom data: {semit:g}\t{semis}
    compressed zeros:           {zerot:g}\t{zeros}'''.format(
        uncomps=tempfiles[0].tell(),
        comps=tempfiles[1].tell(),
        semis=tempfiles[2].tell(),
        zeros=tempfiles[3].tell(),
        uncompt=pstats.Stats(cps[0]).total_tt,
        compt=pstats.Stats(cps[1]).total_tt,
        semit=pstats.Stats(cps[2]).total_tt,
        zerot=pstats.Stats(cps[3]).total_tt
    ))
#+END_SRC
  - floating point numbers are often almost random from a compression algorithm's point of view
  - HDF5's =szip= algorithm is supposed to understand floating point numbers and compress smartly
- *always write huge chunks of data*
  - latency is more likely to ruin performance than anything else, so unless you know exactly where the I/O
    bottleneck is, do big writes into big files, even buffering internally in your code if necessary
  - and big writes really means big: a 10 MB write is not a big write, let alone a big file!
  - unfortunately, python is not very good at demonstrating this but you can try to compile and run this
    (available in =codes/cpp/chunk_size_effect.c=)
#+NAME: chunk_size_effect
#+BEGIN_SRC C :exports none :tangle yes :tangle "../codes/cpp/chunk_size_effect.c" :padline no
  #define _GNU_SOURCE 1
  #define _POSIX_C_SOURCE 200809L
  #define _XOPEN_SOURCE 700
  #include <stdio.h>
  #include <stdlib.h>
  #include <unistd.h>
  #include <time.h>
  #include <sys/types.h>
  #include <sys/stat.h>
  #include <fcntl.h>


  #define SIZE 1000*1000*100

  int main(int argc, char *argv[]) {
    if (argc != 3)
      return 1;
    int fd1 = open(argv[1], O_WRONLY, O_DIRECT|O_TRUNC);
    int fd2 = open(argv[2], O_WRONLY, O_DIRECT|O_TRUNC);
    double *data = (double *) calloc(SIZE, sizeof(double));
    struct timespec t1, t2, t3;
    clock_gettime(CLOCK_MONOTONIC, &t1);
    for (int i=0; i<SIZE; i++) {
      write(fd1, data+i, sizeof(double)*1);
    }
    clock_gettime(CLOCK_MONOTONIC, &t2);
    write(fd2, data, sizeof(double)*SIZE);
    clock_gettime(CLOCK_MONOTONIC, &t3);
    printf("Writing one element at a time took %6li seconds\n", t2.tv_sec-t1.tv_sec);
    printf("Writing all elements at once took  %6li seconds\n", t3.tv_sec-t2.tv_sec);
    return 0;
  }
#+END_SRC
#+BEGIN_SRC sh :exports results :results output verbatim 
cat ../codes/cpp/chunk_size_effect.c
#+END_SRC
#+BEGIN_SRC python
%%bash
cat ../codes/cpp/chunk_size_effect.c
mpicxx -o ../codes/cpp/chunk_size_effect ../codes/cpp/chunk_size_effect.c
f1=$(mktemp ./testXXXXXXXX)
f2=$(mktemp ./testXXXXXXXX)
../codes/cpp/chunk_size_effect $f1 $f2
rm $f1 $f2
#+END_SRC
- Bit of a dark magic as disc, unlike the CPU, is a shared resource: other users use same discs
** Parallel I/O
- always use parallel I/O for parallel programs
- poor man's parallel I/O
  - every worker writes its own file
  - can be the fastest solution
  - but how do you use those files with different number of workers for e.g. post-processing?
- MPI I/O or MPI-enabled HDF5 library deal with that
  - they can write a single file simultaneously from all workers
  - may do some hardware-based optimisations behind the scenes
  - can also map the writes to the MPI topology
  - needs a bit of a learning curve, unless you chose to use h5py or some other library like it which handles
    the complexity for you (e.g. PETSc):
#+BEGIN_SRC python :tangle yes :tangle "../codes/python/petsc_hdf5_viewer.py"
  from __future__ import division
  import sys
  import time
  import numpy
  import mpi4py
  from mpi4py import MPI
  import petsc4py
  petsc4py.init(sys.argv)
  from petsc4py import PETSc
  import tempfile

  dm = PETSc.DMDA().create(dim=3, sizes = (-11,-7,-5),
                           proc_sizes=(PETSc.DECIDE,)*3,
                           boundary_type=(PETSc.DMDA.BoundaryType.GHOSTED,)*3,
                           stencil_type=PETSc.DMDA.StencilType.BOX,
                           stencil_width = 1, dof = 1, comm =
                           PETSc.COMM_WORLD, setup = False)
  dm.setFromOptions()
  dm.setUp()
  vec1 = dm.createGlobalVector()
  vec1.setName("NameOfMyHDF5Dataset")
  vec2 = vec1.duplicate()
  vec2.setName("NameOfMyHDF5Dataset")
  fn = tempfile.NamedTemporaryFile()
  vwr=PETSc.Viewer().createHDF5(fn.name, mode=PETSc.Viewer.Mode.WRITE)
  vec1.view(vwr)
  vwr.destroy()
  vwr=PETSc.Viewer().createHDF5(fn.name, mode=PETSc.Viewer.Mode.READ)
  vec2.load(vwr)
  print("Are they equal? " + ["No!", "Yes!"][vec1.equal(vec2)])
#+END_SRC
  - if you run this in parallel using parallel HDF5 library, you just got all the hard bits for free
  - and a similar example in =h5py=
  - note that running this in the frontend uses just one rank
#+BEGIN_SRC python :tangle yes :tangle "../codes/python/parallel_io_h5py.py"
  import mpi4py
  from mpi4py import MPI
  import h5py
  import tempfile
  import os
  import array
  if (MPI.COMM_WORLD.rank == 0):
      temp="hdf5_visualisation_example.h5"
      temp2=array.array("c","")
      temp2.fromstring(temp)
  else:
      temp2=array.array("c", "\0")*1024
  MPI.COMM_WORLD.Bcast([temp2, MPI.CHAR], root=0)   
  KEEP_ME_AROUND = temp2.tostring()
  rank = MPI.COMM_WORLD.rank
  print KEEP_ME_AROUND                                                  
  f = h5py.File(KEEP_ME_AROUND, "w", driver="mpio", comm=MPI.COMM_WORLD)
  dset = f.create_dataset("test", (4,), dtype="f8")
  dset[rank] = rank
  f.close()
#+END_SRC
#+BEGIN_SRC python
%%bash
mpirun -np 4 python ../codes/python/parallel_io_h5py.py
#+END_SRC
  - performance might still be bad, because
** Know your filesystem
- typical HPDA/HPC system will have a high bandwith, high latency parallel file system where big files should go
- most common is Lustre
  - one often needs to set up a special directory on Lustre for very high bandwidth operations
  - files are /striped/ onto different pieces of hardware (OSTs) to increase bandwidth
  - can be tricky as both the number of active OSTs and number of writers in code
    affect the bandwidth
** Checkpointing
- Your code should be able to do this on its own to support solving the problem by running the code several
  times: often not possible to obtain access to a computer for long enough to solve in one go.
- Basically, you save your iterate or current best estimate solution and later load it from file instead of
  using random or hard coded initial conditions.
